
## self-attention是为了解决什么问题呢？

回想一下我们的MLP模型，它的模型可以抽象为以下的模型。MLP是为了解决输入与输出关系的一个模型。

那么在这里，我们不禁有疑问，如果输入不是一个vector，而是一组vector呢？
输入不是一个表格数据，而是一串数据，如 i saw a saw


那么还是用原来那个模型，可以得到

但是这里的两个saw是不一样的，怎么体现出时间关系呢？

时间关系体现在先后顺序上，

